{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPS+ggCK7yJtQZ0ItDAWbSv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mikosa01/TEAMNOLT/blob/main/Group.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IGj_8AVaf75N"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "LG1LWumxIZ9p"
      },
      "outputs": [],
      "source": [
        "# Code to read csv file into colaboratory:\n",
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "import numpy as np \n",
        "import sys \n",
        "import tensorflow as tf\n",
        "import pandas as pd \n",
        "import matplotlib.pyplot  as plt \n",
        "import zipfile\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import cv2"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "metadata": {
        "id": "3EPXlZsRK3RK"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "downloaded = drive.CreateFile({'id':'17gr_gjOGecUXY_HbDZSFc0U8ADnENMUH'}) # replace the id with id of file you want to access\n",
        "downloaded.GetContentFile('Group_Project_Data 2.zip') "
      ],
      "metadata": {
        "id": "3tCnkutThs2_"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rP5dM41woMS0"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DATADIR = '/content/Group_Project_Data/Train'\n",
        "CATEGORIES = ['Fake', 'Real']"
      ],
      "metadata": {
        "id": "TxDw5CQ_gAhu"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create a function to deal with the dataset\n",
        "\n",
        "training_data = []\n",
        "\n",
        "def create_training_data():\n",
        "  for category in CATEGORIES:\n",
        "    path = os.path.join(DATADIR, category)\n",
        "    classes = CATEGORIES.index(category)\n",
        "    for img in os.listdir(path):\n",
        "      try:\n",
        "        img_array = cv2.imread(os.path.join(path, img), cv2.IMREAD_GRAYSCALE)\n",
        "        # img_array = cv2.imread(os.path.join(path, img))\n",
        "        new_img_array = cv2.resize(img_array, (50, 50))\n",
        "        training_data.append([new_img_array, classes])\n",
        "      except Exception as e:\n",
        "        pass\n",
        "\n",
        "create_training_data()"
      ],
      "metadata": {
        "id": "oG_TPR3MM9t6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "outputId": "c91726ac-3135-45d0-d2c0-deb15bd65667"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-b55434bcd1dc>\u001b[0m in \u001b[0;36m<cell line: 18>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mcreate_training_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-5-b55434bcd1dc>\u001b[0m in \u001b[0;36mcreate_training_data\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDATADIR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcategory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mclasses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCATEGORIES\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcategory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mimg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mimg_array\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIMREAD_GRAYSCALE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/Group_Project_Data/Train/Fake'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "random.shuffle(training_data)\n"
      ],
      "metadata": {
        "id": "D46otCTkgZJb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = []\n",
        "y = []"
      ],
      "metadata": {
        "id": "25CotbjjgeKj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(training_data))"
      ],
      "metadata": {
        "id": "Ul2x0wU7hJzg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for features, label in training_data:\n",
        "  X.append(features)\n",
        "  y.append(label)\n",
        "\n",
        "X = np.array(X).reshape(-1, 50, 50, 1)"
      ],
      "metadata": {
        "id": "Xk54w3yVgeXl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = X/255.0\n",
        "y = np.array(y)"
      ],
      "metadata": {
        "id": "_bdiJPOsgeeo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, Conv2D, MaxPooling2D\n",
        "import pickle"
      ],
      "metadata": {
        "id": "X9xoBA87gvek"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CNN architecture\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Conv2D(64, (3,3), input_shape=X.shape[1:]))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "\n",
        "model.add(Conv2D(64, (3,3)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(64))\n",
        "\n",
        "model.add(Dense(1))\n",
        "model.add(Activation('sigmoid'))"
      ],
      "metadata": {
        "id": "Mrc9SXgTgvWu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "\n",
        "model.fit(X, y, batch_size=32, epochs=10, validation_split=0.1)"
      ],
      "metadata": {
        "id": "k1XLl3eAgvOi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EPA0sx5yk_Lh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import layers, models\n",
        "\n",
        "model = models.Sequential()\n",
        "\n",
        "# Convolutional Layer 1\n",
        "model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=X.shape[1:]))\n",
        "\n",
        "# Max Pooling Layer 1\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "\n",
        "# Convolutional Layer 2\n",
        "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
        "\n",
        "# Max Pooling Layer 2\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "\n",
        "# Flattening Layer\n",
        "model.add(layers.Flatten())\n",
        "\n",
        "# Dense Layer\n",
        "model.add(layers.Dense(128, activation='relu'))\n",
        "\n",
        "# Dropout Layer\n",
        "model.add(layers.Dropout(0.5))\n",
        "\n",
        "# Output Layer\n",
        "model.add(layers.Dense(1, activation='sigmoid'))\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "nPaEAVbjjEY3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "N17pHleBjERn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(X, y, batch_size=32, epochs=10, validation_split=0.1)"
      ],
      "metadata": {
        "id": "tjtrW7MljEEk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "zip_files = ['Train', 'Valid']\n",
        "for zip_file in zip_files:\n",
        "  with zipfile.ZipFile('Group_Project_Data.zip'.format(zip_file), 'r') as z: \n",
        "    z.extractall('.')\n",
        "    print('{} unzipped'.format(zip_file))"
      ],
      "metadata": {
        "id": "XI_yhUogPcez"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bn54r-kZf98k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "#print(os.getcwd())\n",
        "print(os.listdir('/content'))"
      ],
      "metadata": {
        "id": "CNORPU3xPw1S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "IMAGE_FOLDER_PAIR = '/content/Group_Project_Data/Train'    \n",
        "file_names = os.listdir(IMAGE_FOLDER_PAIR)\n",
        "width = 150\n",
        "height = 150  \n",
        "\n",
        "file_names[0:5]"
      ],
      "metadata": {
        "id": "-DRcROk6QIwZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "names = ['Real', 'Fake']\n",
        "targets = []\n",
        "full_path = []\n",
        "train_real_dir = []\n",
        "train_fake_dir = []\n",
        "train_set = []\n",
        "image = []\n",
        "\n",
        "for name in names : \n",
        "  if (name== 'Real'):\n",
        "    path_1 = os.path.join(IMAGE_FOLDER_PAIR,name)\n",
        "    #print(path_1)\n",
        "    file_names = os.listdir(path_1)\n",
        "    #print(file_names)\n",
        "    for file_name in file_names:\n",
        "      path_2 = os.path.join(path_1, file_name)\n",
        "      train_real_dir.append(path_2)\n",
        "      #image.append(file_name)\n",
        "      full_path.append(path_2)\n",
        "      targets.append(name)\n",
        "  elif (name== 'Fake'):\n",
        "    path_1 = os.path.join(IMAGE_FOLDER_PAIR,name)\n",
        "    #print(path_1)\n",
        "    file_names = os.listdir(path_1)\n",
        "    #print(file_names)\n",
        "    for file_name in file_names:\n",
        "      path_2 = os.path.join(path_1, file_name)\n",
        "      train_fake_dir.append(path_2)\n",
        "      #image.append(file_name)\n",
        "      full_path.append(path_2)\n",
        "      targets.append(name)"
      ],
      "metadata": {
        "id": "-6xBjhOzQeJa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(full_path)"
      ],
      "metadata": {
        "id": "O479FvUxpga0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(image)"
      ],
      "metadata": {
        "id": "v6wmgVyKvdfv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train = pd.DataFrame()\n",
        "train['image_path'] = full_path\n",
        "train['target'] = targets"
      ],
      "metadata": {
        "id": "g2sEhcBTW-Pn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('total data counts : ', train['target'].count())\n",
        "counts = train['target'].value_counts()\n",
        "print(counts)"
      ],
      "metadata": {
        "id": "jWhmI1y-XFkN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.image as mpimg\n",
        "rows = 4\n",
        "cols = 4\n",
        "axes = []\n",
        "fig = plt.figure(figsize = (10, 10))\n",
        "i = 0\n",
        "for a in range (rows * cols):\n",
        "  b = mpimg.imread(train_real_dir[i])\n",
        "  sp = fig.add_subplot(rows, cols, a + 1); \n",
        "  axes.append(sp)\n",
        "  sp.axis('off')\n",
        "  plt.imshow(b)\n",
        "  i += 1\n",
        "  plt.tight_layout()\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "HPUJXCdrSDVX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.image as mpimg\n",
        "rows = 4\n",
        "cols = 4\n",
        "axes = []\n",
        "fig = plt.figure(figsize = (10, 10))\n",
        "i = 0\n",
        "for a in range (rows * cols):\n",
        "  b = mpimg.imread(train_fake_dir[i])\n",
        "  sp = fig.add_subplot(rows, cols, a + 1); \n",
        "  axes.append(sp)\n",
        "  sp.axis('off')\n",
        "  plt.imshow(b)\n",
        "  i += 1\n",
        "  plt.tight_layout()\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "avmOcEVTWsUx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "names = ['Real', 'Fake']\n",
        "test_real_dir = []\n",
        "test_fake_dir = []\n",
        "test = []\n",
        "\n",
        "for name in names : \n",
        "  if (name== 'Real'):\n",
        "    path_1 = os.path.join(IMAGE_FOLDER_PAIR,name)\n",
        "    #print(path_1)\n",
        "    file_names = os.listdir(path_1)\n",
        "    #print(file_names)\n",
        "    for file_name in file_names:\n",
        "      path_2 = os.path.join(path_1, file_name)\n",
        "      test_real_dir.append(path_2)\n",
        "      test.append(path_2)\n",
        "  elif (name== 'Fake'):\n",
        "    path_1 = os.path.join(IMAGE_FOLDER_PAIR,name)\n",
        "    #print(path_1)\n",
        "    file_names = os.listdir(path_1)\n",
        "    #print(file_names)\n",
        "    for file_name in file_names:\n",
        "      path_2 = os.path.join(path_1, file_name)\n",
        "      test_fake_dir.append(path_2)\n",
        "      test.append(path_2)"
      ],
      "metadata": {
        "id": "E7Qa8nYLKb-4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "import io\n",
        "image = []\n",
        "for link in :\n",
        "  with open(\"{}\".format(link), \"rb\") as f:\n",
        "    image_bytes = f.read()\n",
        "  image_1 = Image.open(io.BytesIO(image_bytes))\n",
        "\n",
        "  # # Convert the image to PNG format\n",
        "  # png_image_bytes = io.BytesIO()\n",
        "  # image_1.save(png_image_bytes, format=\"PNG\")\n",
        "  # png_image_bytes.seek(0)\n",
        "  \n",
        "\n",
        "  # doc = tf.io.decode_image(png_image_bytes.getvalue(), channels=3) # Assuming a 3-channel color image\n",
        "  # image.append(doc)"
      ],
      "metadata": {
        "id": "hitXTcrU69sl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "import io\n",
        "\n",
        "image = []\n",
        "for link in full_path:\n",
        "  with open(\"{}\".format(link), \"rb\") as f:\n",
        "    image_bytes = f.read()\n",
        "  image_1 = Image.open(io.BytesIO(image_bytes))\n",
        "\n",
        "# Convert the image to a numpy array\n",
        "image_array = np.array(image_1)\n",
        "\n",
        "# Load the image data into a TensorFlow tensor\n",
        "doc = tf.convert_to_tensor(image_array, dtype=tf.float32)"
      ],
      "metadata": {
        "id": "DtwyoA8G8m90"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!pip install -U -q rawkit\n",
        "from io import BytesIO\n",
        "from PIL import Image, ImageFile\n",
        "import numpy\n",
        "from rawkit import raw\n",
        "# def convert_cr2_to_jpg(raw_image):\n",
        "#     raw_image_process = raw.Raw(raw_image)\n",
        "#     buffered_image = numpy.array(raw_image_process.to_buffer())\n",
        "#     if raw_image_process.metadata.orientation == 0:\n",
        "#         jpg_image_height = raw_image_process.metadata.height\n",
        "#         jpg_image_width = raw_image_process.metadata.width\n",
        "#     else:\n",
        "#         jpg_image_height = raw_image_process.metadata.width\n",
        "#         jpg_image_width = raw_image_process.metadata.height\n",
        "#     jpg_image = Image.frombytes('RGB', (jpg_image_width, jpg_image_height), buffered_image)\n",
        "#     return jpg_image\n",
        "\n"
      ],
      "metadata": {
        "id": "6IxgiSQN9ens"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for link in full_path:\n",
        "  full_link = convert_cr2_to_jpg(\"{}\".format(link))"
      ],
      "metadata": {
        "id": "HE79oKH59ftr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for link in train_real_dir:\n",
        "  image = tf.io.read_file('{}'.format(link))\n",
        "  image = tf.io.decode_image(image, channels=3)"
      ],
      "metadata": {
        "id": "xqmMbrM017Ef"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image =tf.io.decode_image(image)\n",
        "test = tf.io.decode_image(test)"
      ],
      "metadata": {
        "id": "NQ9VoVDW1oVQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WPGZeJef1umN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def scaler(doc, label): \n",
        "  doc = tf.cast(doc, tf.float32)\n",
        "  doc /= 255.\n",
        "  return doc, label\n",
        "\n",
        "scaler(image, targets)"
      ],
      "metadata": {
        "id": "uhUPyUcUnNsA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import skimage\n",
        "image = skimage.img_as_float(image)"
      ],
      "metadata": {
        "id": "0KWuYMpmxRoZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4X8L_wZYoyJo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0I_Ch8PbnCzb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}